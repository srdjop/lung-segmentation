# Lung Segmentation in Chest X-Ray Images

## 1. Project Description

This project implements a solution for the automatic segmentation of lung regions in frontal chest X-ray images. The goal was to develop a deep learning model that takes an X-ray image as input and generates a binary mask that accurately outlines the lung fields as output.

This project was developed as part of the "Medical Image Analysis" course.

### Example Result
<p align="center">
  <img src="[https://path/to/your/example/image.png](https://github.com/srdjop/lung-segmentation/blob/main/outputs/predicted_CHNCXR_0022_0.png)" width="500" />
  <br />
  <em>Example segmentation generated by the `inference.py` script.</em>
</p>

## 2. Methodology

The proposed **SAM (Segment Anything Model)** architecture was initially investigated. After extensive testing of various fine-tuning strategies (adjusting learning rates, different prompt types, unfreezing layers), it was concluded that the model exhibited training instability on this specific medical dataset. This journey was documented through multiple experiments tracked with Weights & Biases.

Therefore, a decision was made to switch to a more robust approach, proven in the domain of medical image segmentation: a **U-Net architecture with a pre-trained ResNet34 encoder**, using the `segmentation-models-pytorch` library. This approach yielded significantly better and more stable results, outperforming a baseline U-Net model.

The final model achieved a **Dice Score of 0.958** on the validation set.

## 3. Repository Structure

```
lung_segmentation_project/
├── data/                    # For storing the dataset (not included in the repo)
├── notebooks/
│   └── 1_data_exploration.ipynb # Jupyter Notebook for data analysis and visualization
├── outputs/                 # Directory for inference results (created automatically)
├── scripts/
│   └── upload_model.py      # Helper script to upload the model to Hugging Face Hub
├── src/
│   ├── data_loader.py       # Custom PyTorch Dataset and augmentation functions
│   ├── train_unet.py        # Main script for training the final U-Net model
│   ├── inference.py         # Script to run inference on new images
│   └── sanity_check_unet.py # Script for the U-Net model's sanity check (overfitting)
├── .gitignore               # Specifies intentionally untracked files to ignore
├── README.md                # This file
└── requirements.txt         # List of all required Python libraries
```

## 4. Setup and Installation

### Step 1: Clone the Repository
```bash
git clone https://github.com/srdjop/lung-segmentation.git
cd lung-segmentation
```

### Step 2: Create and Activate a Virtual Environment
Using a virtual environment is highly recommended to isolate project dependencies.
```bash
# Create the environment
python -m venv venv

# Activate on Windows
.\venv\Scripts\activate

# Activate on macOS/Linux
source venv/bin/activate
```

### Step 3: Install Dependencies
**Note**: *torch* and *torchvision* are not included in requirements.txt due to their specific installation requirements. You must install them manual after the other dependencies.
All required libraries are listed in the `requirements.txt` file.
```bash
pip install -r requirements.txt
```
- **For user with an NVIDIA GPU:**
Install PyTorch with CUDA support. Visit the official PyTorch website https://pytorch.org/get-started/locally/ for the most precise command. Example: 
```bash
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126
```
- **For user without a GPU or with an AMD/Apple GPU**
Install the PyTorch CPU-only version. Example:
```bash
pip3 install torch torchvision
```

### Step 4: Download the Dataset
The dataset used for training is not included in this repository.
1.  Download the data from the following link: **https://drive.google.com/file/d/1cZ30dzi6u9HQRQn7AGiKJeCmYKaznTT0/view?usp=drive_link**.
2.  Create the `data` directories inside the project.
3.  Unpack the data to match the following structure:
    ```
    data/
    ├── images/
    │   ├── image001.png
    │   └── ...
    └── masks/
        ├── image001_mask.png
        └── ...
    ```

## 5. Usage

### Running Inference (Main Demonstration)
The inference script automatically downloads the pre-trained model from my Hugging Face Hub. To test the model on a random image from the dataset, run:
```bash
python src/inference.py
```
The result (an image with the segmented mask overlay) will be saved in the `outputs/` directory.

### Training the Model (Optional)
If you wish to retrain the model from scratch:

**1. Log in to Weights & Biases (for tracking):**
```bash
wandb login
```
(You will need an API key from your `wandb.ai` account.)

**2. Run the training script:**
```bash
python src/train_unet.py
```
The best-performing model will be saved in the project's root directory as `best_unet_model.pth`.

### Uploading a Model to Hugging Face Hub (Optional)
If you have retrained a model and want to upload it to your Hugging Face Hub:

**1. Log in to Hugging Face:**
```bash
huggingface-cli login
```

**2. Update and run the upload script:**
*   Open `scripts/upload_model.py` and change `HF_REPO_ID` to your own repository name.
*   Run the script:
    ```bash
    python scripts/upload_model.py
    ```
